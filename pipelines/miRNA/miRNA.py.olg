#!/usr/bin/env python

#LICENSE AND COPYRIGHT

#Copyright (C) 2015 National Research Council Canada

#This license does not grant you the right to use any trademark, service
#mark, tradename, or logo of the Copyright Holder.

#This license includes the non-exclusive, worldwide, free-of-charge
#patent license to make, have made, use, offer to sell, sell, import and
#otherwise transfer the Package with respect to any patent claims
#licensable by the Copyright Holder that are necessarily infringed by the
#Package. If you institute patent litigation (including a cross-claim or
#counterclaim) against any party alleging that the Package constitutes
#direct or contributory patent infringement, then this Artistic License
#to you shall terminate on the date that such litigation is filed.

#Disclaimer of Warranty: THE PACKAGE IS PROVIDED BY THE COPYRIGHT HOLDER
#AND CONTRIBUTORS "AS IS' AND WITHOUT ANY EXPRESS OR IMPLIED WARRANTIES.
#THE IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR
#PURPOSE, OR NON-INFRINGEMENT ARE DISCLAIMED TO THE EXTENT PERMITTED BY
#YOUR LOCAL LAW. UNLESS REQUIRED BY LAW, NO COPYRIGHT HOLDER OR
#CONTRIBUTOR WILL BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, OR
#CONSEQUENTIAL DAMAGES ARISING IN ANY WAY OUT OF THE USE OF THE PACKAGE,
#EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

#Author: Julien Tremblay - julien.tremblay@nrc-cnrc.gc.ca

# Python Standard Modules
import argparse
import collections
import logging
import os
import re
import sys
import errno

# Append nrc_pipeline directory to Python library path
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(sys.argv[0]))))

# NRC Modules
from core.config import *
from core.job import *
from core.pipeline import *
#from bio.design import *
from bio.readset import *

from bio import shotgun_metagenomics
from bio import microbial_ecology

from pipelines.illumina import illumina

# Global scope variables
log = logging.getLogger(__name__)

class MiRNA(illumina.Illumina):

    def trim(self):
        jobs = []
        # Merge all demultiplexed fastq files in one file. One file for reads1 and one file for reads2 of Illumina paired-end.
        # If library is Illumina single end or 454 or PacBio, only one file will be generated.
        outdir = self._root_dir
        trim_stats = []
        #sys.stderr.write('outdir: ' + outdir + '\n')
        
        for readset in self.readsets:
            trim_file_prefix = os.path.join(outdir, "qced_reads", readset.sample.name, readset.name + ".trim.")
            trim_file_prefix_long = os.path.join(outdir, "qced_reads", readset.sample.name, readset.name + ".trim.long.")
            if not os.path.exists(trim_file_prefix):
                os.makedirs(trim_file_prefix)
            
            if readset.run_type == "PAIRED_END":
                #sys.stderr.write('outdir: ' + readset.fastq1 + '\n')

                job = shotgun_metagenomics.trimmomatic(
                    readset.fastq1,
                    readset.fastq2,
                    trim_file_prefix + "pair1.fastq.gz",
                    trim_file_prefix + "single1.fastq.gz",
                    trim_file_prefix + "pair2.fastq.gz",
                    trim_file_prefix + "single2.fastq.gz",
                    readset.quality_offset,
                    trim_file_prefix + "out",
                    trim_file_prefix + "stats.csv"
                )
                job.name = "trimmomatic_" + readset.sample.name
                job.subname = "trim"
                jobs.append(job) 

                trim_stats.append(trim_file_prefix + "stats.csv")
            
                # Merge R1 and R2 to get an interleaved file. We do this here because of memory requirements
                # for downstream duk steps. Here we assume that R1 and R2 are in the same exact order.
                job = shotgun_metagenomics.create_interleaved_fastq(
                    trim_file_prefix + "pair1.fastq.gz",
                    trim_file_prefix + "pair2.fastq.gz",
                    trim_file_prefix + "interleaved.fastq",
                    trim_file_prefix + "interleaved.fastq.gz"
                )
                job.name = "create_interleaved_fastq_" + readset.sample.name
                job.subname = "interleaved_fastq"
                jobs.append(job) 

            elif readset.run_type == "SINGLE_END" :
                if config.param("DEFAULT", "qc_methods", 1, "string") == "trimmomatic":

                    job = shotgun_metagenomics.trimmomatic_se(
                        readset.fastq1,
                        trim_file_prefix + "pair1.fastq.gz",
                        readset.quality_offset,
                        trim_file_prefix + "out",
                        trim_file_prefix + "stats.csv"
                    )
                    job.name = "trimmomatic_" + readset.sample.name
                    job.subname = "trim"
                    jobs.append(job) 
    
                    trim_stats.append(trim_file_prefix + "stats.csv")
                    
                    job = shotgun_metagenomics.filter_max_length(
                        trim_file_prefix + "pair1.fastq.gz",
                        trim_file_prefix + "pair1_lf.fastq.gz" #lf = length filtered
                    )
                    job.name = "filter_max_length_" + readset.sample.name
                    job.subname = "DEFAULT"
                    jobs.append(job) 
            
            else:
                raise Exception("Error: run type \"" + readset.run_type +
                "\" is invalid for readset \"" + readset.name + "\" (should be PAIRED_END or SINGLE_END)!")
        
        return jobs
            
    def duk(self):
        jobs=[]
        outdir = self._root_dir
        logs = []
        readset_ids = []

        for readset in self.readsets:
            trim_file_prefix = os.path.join(outdir, "qced_reads", readset.sample.name, readset.name + ".trim.")
            trim_file_prefix_long = os.path.join(outdir, "qced_reads", readset.sample.name, readset.name + ".trim.long.")
            outfile_prefix = os.path.join(outdir, "qced_reads", readset.sample.name, readset.name + ".")
            outfile_prefix_long = os.path.join(outdir, "qced_reads", readset.sample.name, readset.name + ".long.")
            out1up = os.path.join(self._root_dir, "qced_reads", readset.sample.name, readset.name + ".ncontam_unpaired_R1.fastq.gz")
            out2up = os.path.join(self._root_dir, "qced_reads", readset.sample.name, readset.name + ".ncontam_unpaired_R2.fastq.gz")
            out1p = os.path.join(self._root_dir, "qced_reads", readset.sample.name, readset.name + ".ncontam_paired_R1.fastq.gz")
            out2p = os.path.join(self._root_dir, "qced_reads", readset.sample.name, readset.name + ".ncontam_paired_R2.fastq.gz")
            
            if readset.run_type == "PAIRED_END":
                log = os.path.join(self._root_dir, "qced_reads", readset.sample.name, readset.name + ".duk_contam_interleaved_log.txt")
                readset_id = readset.name

                job = shotgun_metagenomics.duk_gz(
                    trim_file_prefix + "interleaved.fastq.gz",
                    outfile_prefix + "contam.fastq",
                    outfile_prefix + "ncontam.fastq.gz",
                    log,
                    config.param('DB', 'contaminants', 1, 'filepath')
                )
                job.name = "duk_interleaved_" + readset.sample.name
                job.subname = "duk"
                jobs.append(job)
            
                job = shotgun_metagenomics.remove_unpaired_reads_and_split(
                    outfile_prefix + "ncontam.fastq.gz",
                    out1up,
                    out2up,
                    out1p,
                    out2p,
                    outfile_prefix + "ncontam_paired.fastq.gz"
                )
                job.name = "remove_unpaired_and_split_" + readset.sample.name
                job.subname = "remove_unpaired"
                jobs.append(job)
                
                logs.append(log)
                readset_ids.append(readset.name)
            
            elif readset.run_type == "SINGLE_END":
                log = os.path.join(self._root_dir, "qced_reads", readset.sample.name, readset.name + ".duk_contam_pair1_log.txt")

                job = shotgun_metagenomics.duk_gz(
                    trim_file_prefix + "pair1.fastq.gz",
                    outfile_prefix + "contam.fastq",
                    outfile_prefix + "ncontam.fastq.gz",
                    log,
                    config.param('DB', 'contaminants', 1, 'filepath')
                )
                job.name = "duk_single_end_reads_" + readset.sample.name
                job.subname = "duk"
                jobs.append(job)
                
                logs.append(log)
                readset_ids.append(readset.name)
            
        # Compile duk logs.
        job = shotgun_metagenomics.merge_duk_logs_interleaved(
            logs,
            readset_ids,
            os.path.join(self._root_dir, "qced_reads", "duk_merged_logs.tsv")
        )
        job.name = "merge_duk_logs"
        job.subname = "merge_duk_logs"
        jobs.append(job)
            
        return jobs
     
    
    def cleanup(self):
        #Here, compress all .fastq files into .fastq.gz.
        jobs = []
        #job = shotgun_metagenomics.mymethod(
        #)
        #job.name = "myjobname"
        #jobs.append(job)
        sys.stderr.write('not implemented yet\n')
        return jobs
    
    # Override illumina.py readsets to make sure we are parsing a nanuq sample sheet
    # and not a readset sheet.
    @property
    def readsets(self):
        if not hasattr(self, "_readsets"):
            self._readsets = parse_nanuq_readset_file(self.args.readsets.name)
        return self._readsets

    @property
    def steps(self):
        
        return [
            # Core steps.
            self.trim,#1
            self.duk,#2
            self.cleanup#21
        ]

    def set_local_variables(self):
        self._parser_local = argparse.ArgumentParser(description='Process options.')
        self._parser_local.add_argument("-c", "--config", help="config INI-style file", nargs="+", type=file, required=True)
        self._parser_local.add_argument("-r", "--readsets", help="readset file", type=file, required=False)
        self._parser_local.add_argument("-s", "--steps", help="step range e.g. '1-5', '3,6,7', '2,4-8'", required=True)
        self._parser_local.add_argument("-o", "--output-dir", help="output directory (default: current)", default=os.getcwd())
        self._parser_local.add_argument("-j", "--job-scheduler", help="job scheduler type (default: slurm)", choices=["torque", "batch", "daemon", "slurm"], default="slurm")
        self._parser_local.add_argument("-f", "--force", help="force creation of jobs even if up to date (default: false)", action="store_true")
        self._parser_local.add_argument("-l", "--log", help="log level (default: info)", choices=["debug", "info", "warning", "error", "critical"], default="info")
        self._parser_local.add_argument("-z", "--json", help="generate pipeline path in json format", default=sys.stdout, type=argparse.FileType('w'), required=False)

        # barcodes
        self._args_local = self._parser_local.parse_args()
        config.parse_files(self._args_local.config)
        self._config = self._args_local.config[0]
        self._config = os.path.abspath(self._config.name)
        
        self._root_dir = self._args_local.output_dir
        if not os.path.exists(self._root_dir):
            os.makedirs(self._root_dir)
        
        # Make directories
        self.make_directories(self._root_dir)
  
    # Define and make directories. Also desing initial infile.
    def make_directories(self, root_dir):
        def mkdir_p(path):
            try:
                os.makedirs(path)
            except OSError as exc: # Python >2.5
                if exc.errno == errno.EEXIST and os.path.isdir(path):
                    pass
                else: raise
         
        mkdir_p(root_dir)
        mkdir_p(os.path.join(root_dir, "qced_reads"))
    
    @property
    def readsets(self):
        if not hasattr(self, "_readsets"):
            self._readsets = parse_readset_file(self.args.readsets.name)
        return self._readsets


    def __init__(self):
        # Add pipeline specific arguments
        self.set_local_variables()
        sys.stderr.write('MiRNA pipeline\n')
        super(MiRNA, self).__init__()
                
MiRNA().submit_jobs()
